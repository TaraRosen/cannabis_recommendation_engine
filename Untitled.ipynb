{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keerthan Ekbote\n",
    "saiskee\n",
    "\n",
    "def strain_scraper():\n",
    "    #A quick look at https://www.leafly.com/explore/page-56/sort-alpha shows \n",
    "    #that this is the last page of strains\n",
    "    pages = 56 \n",
    "    name_file = open(\"strain_names.txt\",\"a+\")\n",
    "    \n",
    "    for i in range(pages+1):\n",
    "        page_url = \"https://www.leafly.com/explore/page-\"+str(i)+\"/sort-alpha\"\n",
    "        html = requests.get(page_url).text\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        strain_tiles = soup.findAll(\"a\",{\"class\":\"ga_Explore_Strain_Tile\"})\n",
    "        \n",
    "        for tile in strain_tiles:\n",
    "            link = tile.findChildren(\"text\",{\"ng-if\":\"!name\"})[0].findAll(text=True)\n",
    "            \n",
    "            for item in link:\n",
    "                if item == \"\\n\":\n",
    "                    link.remove(item)\n",
    "            output = (tile.get('href')+\": \" + link[0]).encode('ascii',errors='ignore').decode()\n",
    "            name_file.write(output + \"\\n\")\n",
    "    name_file.close()\n",
    "    \n",
    "     for tile in strain_tiles:\n",
    "            link = tile.findChildren(\"text\",{\"ng-if\":\"!name\"})[0].findAll(text=True)\n",
    "            \n",
    "            for item in link:\n",
    "                if item == \"\\n\":\n",
    "                    link.remove(item)\n",
    "            output = (tile.get('href')+\": \" + link[0]).encode('ascii',errors='ignore').decode()\n",
    "            name_file.write(output + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strain_scraper():\n",
    "    #A quick look at https://www.leafly.com/explore/page-56/sort-alpha shows that this is the last page of strains\n",
    "    pages = 56 \n",
    "    name_file = open(\"strain_names.txt\",\"a+\")\n",
    "    \n",
    "    for i in range(pages+1):\n",
    "        page_url = \"https://www.leafly.com/explore/page-\"+str(i)+\"/sort-alpha\"\n",
    "        html = requests.get(page_url).text\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        strain_tiles = soup.findAll(\"a\",{\"class\":\"ga_Explore_Strain_Tile\"})\n",
    "        \n",
    "        for tile in strain_tiles:\n",
    "            link = tile.findChildren(\"text\",{\"ng-if\":\"!name\"})[0].findAll(text=True)\n",
    "            \n",
    "            for item in link:\n",
    "                if item == \"\\n\":\n",
    "                    link.remove(item)\n",
    "            output = (tile.get('href')+\": \" + link[0]).encode('ascii',errors='ignore').decode()\n",
    "            name_file.write(output + \"\\n\")\n",
    "    name_file.close()\n",
    "\n",
    "num_pages = 57\n",
    "if __name__ == '__main__':\n",
    "    p = Pool(20)\n",
    "    records = p.map(weedStrainScraper, list(range(num_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import requests\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "date = datetime.datetime.now()\n",
    "\n",
    "def weedStrainScraper():\n",
    "    weed_file = open(\"weed_file.txt\",\"a+\")\n",
    "    for i in range(57):\n",
    "        html = requests.get(\"https://www.leafly.com/explore/page-\"+str(i)+\"/sort-alpha\").text\n",
    "        soup = BeautifulSoup(html,'html.parser')\n",
    "        a = soup.findAll(\"a\",{\"class\":\"ga_Explore_Strain_Tile\"})\n",
    "        for link in a:\n",
    "            b = link.findChildren(\"text\",{\"ng-if\":\"!name\"})\n",
    "            c = b[0].findAll(text=True)\n",
    "            for item in c:\n",
    "                if item == \"\\n\":\n",
    "                    c.remove(item)\n",
    "            output = (link.get('href')+\": \" + c[0]).encode('ascii',errors='ignore').decode()\n",
    "            weed_file.write(output + \"\\n\")\n",
    "    weed_file.close()\n",
    "\n",
    "\n",
    "def reviewLinkScraper(url):\n",
    "    result = []\n",
    "    \n",
    "    for i in range(10000):\n",
    "        # print(psutil.cpu_percent())\n",
    "        print(url+\"?page=\"+str(i)+\"&sort=rating\")\n",
    "        html1 = requests.get(url+\"?page=\"+str(i)+\"&sort=rating\").text\n",
    "        soup1 = BeautifulSoup(html1, 'html.parser')\n",
    "        review_divs = soup1.findAll(\"div\", {\"class\": \"m-review\"})\n",
    "\n",
    "        if len(review_divs) == 0:\n",
    "            print(\"hello\")\n",
    "            break\n",
    "        for div in review_divs:\n",
    "            username = div.findChild(\n",
    "                \"h3\", {\"class\": \"copy--xl padding-rowItem no-margin\"})\n",
    "            review = div.findChild(\"p\", {\"class\": \"copy--xs copy-md--md\"})\n",
    "            full_review_link = div.findChild(\n",
    "                \"a\", {\"class\": \"copy--xs copy-md--md\"})\n",
    "            output = username.getText().replace(\"\\n\", \"\") + \",\" + \"https://www.leafly.com\" + \\\n",
    "                full_review_link.get(\n",
    "                    'href') + \",\\\"\"+review.getText().encode('ascii', errors='ignore').decode() + \"\\\"\"\n",
    "            result.append(output.encode('ascii', errors='ignore').decode())\n",
    "            # 9 reviews per page\n",
    "    return result\n",
    "\n",
    "\n",
    "def getUrl(lines):\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        linearray = line.split(\":\")\n",
    "        result.append(\"https://www.leafly.com\" +\n",
    "                    linearray[0].strip() + \"/reviews\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def writeToTextfile(line):\n",
    "    linearray = line.split(\":\")\n",
    "    print(\"Scraping \"+linearray[1].strip())\n",
    "    url = \"https://www.leafly.com\"+linearray[0].strip()+\"/reviews\"\n",
    "    print(\"URLSCRAPE \" + url)\n",
    "    strain = linearray[0].split('/')[2]\n",
    "    fileurl = \"results/\"+strain+\".csv\"\n",
    "    output_text = open(fileurl, \"w\")\n",
    "    array = reviewLinkScraper(url)\n",
    "    output_text.write(linearray[1].strip() + \"\\n\" + linearray[0].strip() + \"\\n\")\n",
    "    output_text.write(\"=============\" + \"\\n\")\n",
    "    output_text.close()\n",
    "    file = open(fileurl, \"a+\")\n",
    "    for elem in array:\n",
    "        file.write(elem + \"\\n\")\n",
    "    file.close()\n",
    "\n",
    "\n",
    "#run the below method to collect strains into weed-file.txt\n",
    "# weedStrainScraper()\n",
    "\n",
    "#run the below method to \n",
    "threads = 1\n",
    "weedfile = open(\"weed_file.txt\", \"r\")\n",
    "lines = weedfile.readlines()\n",
    "if __name__ == '__main__':\n",
    "    p = Pool(threads)\n",
    "    records = p.map(writeToTextfile, lines)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
